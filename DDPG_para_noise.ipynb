{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Policy Gradients (DDPG and REINFORCE)\n",
    "\n",
    "Name:Lianming Shi\n",
    "\n",
    "ID:A99097650"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "This exercise requires you to solve various continous control problems in OpenAI-Gym.  \n",
    "\n",
    "DDPG is policy gradient actor critic method for continous control which is off policy. It tackles the curse of dimensionality / loss of performance faced when discretizing a continous action domain. DDPG uses similiar \"tricks\" as DQN to improve the stability of training, including a replay buffer and target networks.\n",
    "\n",
    "Furthermore, you will implement REINFORCE for discrete and continous environments, and as a bonus compare the sample efficiency and performance with DQN and DDPG.\n",
    "\n",
    "\n",
    "### DDPG paper: https://arxiv.org/pdf/1509.02971.pdf\n",
    "\n",
    "### Environments:\n",
    "\n",
    "#### InvertedPendulum-v2 environment:\n",
    "<img src=\"inverted_pendulum.png\" width=\"300\">\n",
    "\n",
    "#### Pendulum-v0 environment:\n",
    "<img src=\"pendulum.png\" width=\"300\">\n",
    "\n",
    "#### Halfcheetah-v2 environment:\n",
    "<img src=\"half_cheetah.png\" width=\"300\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup environment for Actor Critic\n",
    "- inline plotting\n",
    "- gym\n",
    "- directory for logging videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "#environment\n",
    "import gym\n",
    "import os\n",
    "import time\n",
    "#pytorch\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "logging_interval = 40\n",
    "animate_interval = logging_interval * 5\n",
    "logdir='./DDPG/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up gym environment\n",
    "The code below does the following for you:\n",
    "- Wrap environment, log videos, setup CUDA variables (if GPU is available)\n",
    "- Record action and observation space dimensions\n",
    "- Fix random seed for determinisitic training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-23 18:24:43,614] Making new env: InvertedPendulum-v1\n"
     ]
    }
   ],
   "source": [
    "VISUALIZE = False\n",
    "SEED = 0\n",
    "MAX_PATH_LENGTH = 500\n",
    "NUM_EPISODES = 12000\n",
    "GAMMA=0.99\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Environments to be tested on\n",
    "env_name = 'InvertedPendulum-v1'\n",
    "#env_name = 'Pendulum-v0'\n",
    "#env_name = 'HalfCheetah-v1' \n",
    "\n",
    "# wrap gym to save videos\n",
    "env = gym.make(env_name)\n",
    "if VISUALIZE:\n",
    "    if not os.path.exists(logdir):\n",
    "        os.mkdir(logdir)\n",
    "    env = gym.wrappers.Monitor(env, logdir, force=True, video_callable=lambda episode_id: episode_id%logging_interval==0)\n",
    "env._max_episodes_steps = MAX_PATH_LENGTH\n",
    "\n",
    "# check observation and action space\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "\n",
    "if discrete:\n",
    "    print(\"This is a discrete action space, probably not the right algorithm to use\")\n",
    "\n",
    "# set random seeds\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# make variable types for automatic setting to GPU or CPU, depending on GPU availability\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate your understanding of the simulation:\n",
    "For the environments mentioned above ('Pendulum-v0', 'HalfCheetah-v2', 'InvertedPendulum-v2'),\n",
    "- describe the reward system\n",
    "- describe the each state variable (observation space)\n",
    "- describe the action space\n",
    "- when is the environment considered \"solved\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement an action normalization class:\n",
    "To train across various environments, it is useful to normalize action inputs and outputs between [-1, 1]. This class should take in actions and implement forward and reverse functions to map actions between [-1, 1] and [action_space.low, action_space.high].\n",
    "\n",
    "Using the following gym wrapper, implement this class.\n",
    "- https://github.com/openai/gym/blob/78c416ef7bc829ce55b404b6604641ba0cf47d10/gym/core.py\n",
    "- i.e. we are overriding the outputs scale of actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeAction(gym.ActionWrapper):\n",
    "    def action(self, action):\n",
    "        action = (action + 1) / 2  \n",
    "        action *= (self.action_space.high - self.action_space.low)\n",
    "        action += self.action_space.low\n",
    "        return action\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        action -= self.action_space.low\n",
    "        action /= (self.action_space.high - self.action_space.low)\n",
    "        a\n",
    "        ction = action * 2 - 1\n",
    "        return actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a weight syncing function\n",
    "In contrast to DQN, DDPG uses soft weight sychronization. At each time step following training, the actor and critic target network weights are updated to track the rollout networks. \n",
    "- target_network.weights <= target_network.weights \\* (1 - tau) + source_network.weights \\* (tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightSync(target_model, source_model, tau = 0.001):\n",
    "    for parameter_target, parameter_source in zip(target_model.parameters(), source_model.parameters()):\n",
    "        parameter_target.data.copy_((1 - tau) * parameter_target.data + tau * parameter_source.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Replay class that includes all the functionality of a replay buffer\n",
    "DDPG is an off policy actor-critic method and an identical replay buffer to that used for the previous assignment is applicable here as well (do not include the generate_minibatch method in your Replay class this time). Like before, your constructor for Replay should create an initial buffer of size 1000 when you instantiate it.\n",
    "\n",
    "The replay buffer should kept to some maximum size (60000), allow adding of samples and returning of samples at random from the buffer. Each sample (or experience) is formed as (state, action, reward, next_state, done). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class Replay(object):\n",
    "    def __init__(self, maxlen = 60000):\n",
    "        self.maxlen = maxlen\n",
    "        #self.data = deque(maxlen = self.maxlen)\n",
    "        self.data=[]\n",
    "        self.position=0\n",
    "\n",
    "        self.initialize(init_length=1000, envir=env)\n",
    "    def initialize(self,init_length=1000, envir=env):\n",
    "        s = envir.reset()\n",
    "        for i in range (init_length):\n",
    "            #a = np.random.random(1)-np.random.random(1)\n",
    "            a = env.action_space.sample()\n",
    "            s1, r, done, _ = env.step(a)            \n",
    "            self.add([np.reshape(s,(1,obs_dim)), np.reshape(a,(1,act_dim)), r, done, np.reshape(s1,(1,obs_dim))])\n",
    "                        \n",
    "            if done:\n",
    "                s=envir.reset()\n",
    "            else: \n",
    "                s = s1  \n",
    "             \n",
    "         \n",
    "    def add(self, ep):\n",
    "        self.data.append(ep)\n",
    "        self.position = (self.position + 1) % self.maxlen       \n",
    "        #self.data[self.position] = tuple(ep)\n",
    "        \n",
    "    def sample(self, nsamples):\n",
    "        if nsamples > len(self.data):\n",
    "            return random.sample(self.data, len(self.data))\n",
    "        else:\n",
    "            return random.sample(self.data, nsamples)\n",
    "\n",
    "    def display(self):\n",
    "        for x in self.data:\n",
    "            print (x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write an Ornstein Uhlenbeck process class for exploration noise\n",
    "The proccess is described here:\n",
    "- https://en.wikipedia.org/wiki/Ornstein–Uhlenbeck_process\n",
    "- http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "\n",
    "You should implement:\n",
    "- a step / sample method\n",
    "- reset method\n",
    "\n",
    "Use theta = 0.15, mu = 0, sigma = 0.3, dt = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeckProcess:\n",
    "    def __init__(self, mu=np.zeros(act_dim), sigma=0.05, theta=.25, dimension=1e-2, x0=None,num_steps=12000):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dimension\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def step(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Deep Neural Network class that creates a dense network of a desired architecture for actor and critic networks\n",
    "\n",
    "\n",
    "#### Actor\n",
    "- input and hidden layer activation function: ReLU\n",
    "\n",
    "- output activation function: Tanh\n",
    "\n",
    "- hidden_state sizes: 400\n",
    "\n",
    "- state and action sizes: variable\n",
    "\n",
    "- number of hidden layers: 2\n",
    "\n",
    "- batch normalization applied to all hidden layers\n",
    "\n",
    "- weight initialization: normal distribution with small variance. \n",
    "\n",
    "#### Critic\n",
    "- input and hidden layer activation function: ReLU\n",
    "\n",
    "- output activation function: None\n",
    "\n",
    "- hidden_state sizes: 300, 300 + action size\n",
    "\n",
    "- state and action sizes: variable\n",
    "\n",
    "- number of hidden layers: 2\n",
    "\n",
    "- batch normalization applied to all hidden layers prior to the action input\n",
    "\n",
    "- weight initialization: normal distribution with small variance.\n",
    "\n",
    "Good baselines can be found in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# actor model, MLP\n",
    "# ----------------------------------------------------\n",
    "# 2 hidden layers, 400 units per layer, tanh output to bound outputs between -1 and 1\n",
    "\n",
    "class critic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size,output_size = 1):\n",
    "        super(critic, self).__init__()\n",
    "\n",
    "        self.state_dim = state_size\n",
    "        self.action_dim = action_size\n",
    "        h1_dim = 300\n",
    "        h2_dim = 300\n",
    "\n",
    "        self.fc1 = nn.Linear(self.state_dim,h1_dim)\n",
    "        \n",
    "        #self.bn1 = nn.BatchNorm1d(h1_dim)\n",
    "        \n",
    "        self.fc2 = nn.Linear(h1_dim + self.action_dim,h2_dim)\n",
    "        \n",
    "        self.fc3 = nn.Linear(h2_dim,output_size)\n",
    "\n",
    "\n",
    "    def forward(self, sa):\n",
    "        state, action =sa\n",
    "        #s1 = F.relu(self.bn1(self.fc1(state)))\n",
    "        s1 = F.relu(self.fc1(state))\n",
    "\n",
    "        x = torch.cat((s1,action),dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class actor(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(actor, self).__init__()\n",
    "\n",
    "        self.state_dim = input_size\n",
    "        self.action_dim = output_size\n",
    "        h1_dim = 400\n",
    "        h2_dim = 400\n",
    "\n",
    "        self.fc1 = nn.Linear(self.state_dim,h1_dim)\n",
    "        #self.bn1 = nn.BatchNorm1d(h1_dim)\n",
    "\n",
    "        self.fc2 = nn.Linear(h1_dim,h2_dim)\n",
    "        #self.bn2 = nn.BatchNorm1d(h2_dim)\n",
    "\n",
    "        self.fc3 = nn.Linear(h2_dim,self.action_dim)\n",
    "\n",
    "        \n",
    "    def forward(self, state):\n",
    "        #x = F.relu(self.bn1(self.fc1(state)))\n",
    "        #x = F.relu(self.bn2(self.fc2(x)))\n",
    "        \n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        action = F.tanh(self.fc3(x))\n",
    "\n",
    "        return action\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define DDPG class to encapsulate definition, rollouts, and training\n",
    "\n",
    "- gamma = 0.99\n",
    "\n",
    "- actor_lr = 1e-4\n",
    "\n",
    "- critic_lr = 1e-3\n",
    "\n",
    "- critic l2 regularization = 1e-2\n",
    "\n",
    "- noise decay\n",
    "\n",
    "- noise class\n",
    "\n",
    "- batch_size = 128\n",
    "\n",
    "- optimizer: Adam\n",
    "\n",
    "- loss (critic): mse\n",
    "\n",
    "Furthermore, you can experiment with action versus parameter space noise. The standard implimentation works with action space noise, howeve parameter space noise has shown to produce excellent results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    def __init__(self, obs_dim, act_dim, critic_lr = 1e-3, actor_lr = 1e-4, gamma = GAMMA, batch_size = BATCH_SIZE):\n",
    "        \n",
    "        self.gamma = GAMMA\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        \n",
    "        # actor\n",
    "        self.actor = actor(input_size = obs_dim, output_size = act_dim).type(FloatTensor)\n",
    "        self.actor_perturbed = actor(input_size = obs_dim, output_size = act_dim).type(FloatTensor)\n",
    "        self.actor_target = actor(input_size = obs_dim, output_size = act_dim).type(FloatTensor)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "        # critic\n",
    "        self.critic = critic(state_size = obs_dim, action_size = act_dim, output_size = 1).type(FloatTensor)\n",
    "        self.critic_target = critic(state_size = obs_dim, action_size = act_dim, output_size = 1).type(FloatTensor)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        if use_cuda:\n",
    "            self.actor.cuda()\n",
    "            self.actor_target.cuda()\n",
    "            self.critic.cuda()\n",
    "            self.critic_target.cuda()\n",
    "        # optimizers\n",
    "        self.optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr = actor_lr)\n",
    "        self.optimizer_critic = torch.optim.Adam(self.critic.parameters(), lr = critic_lr, weight_decay=1e-2)\n",
    "        \n",
    "        # critic loss\n",
    "        self.critic_loss = nn.MSELoss()\n",
    "        \n",
    "        # noise\n",
    "        self.noise = OrnsteinUhlenbeckProcess(dimension = act_dim, num_steps = NUM_EPISODES)\n",
    "\n",
    "        # replay buffer \n",
    "        self.replayBuffer = Replay()\n",
    "\n",
    "        \n",
    "    def action(self, s, noise, para):\n",
    "        obs = torch.from_numpy(s).unsqueeze(0)\n",
    "        inp = Variable(obs,requires_grad=False).type(FloatTensor)\n",
    "\n",
    "        self.actor.eval()\n",
    "        self.actor_perturbed.eval()\n",
    "\n",
    "        if para is not None:\n",
    "            a = self.actor_perturbed(inp).data[0].cpu().numpy() \n",
    "        else:\n",
    "            a = self.actor(inp).data[0].cpu().numpy() \n",
    "        self.actor.train()\n",
    "\n",
    "        if noise is not None:\n",
    "            a = a + noise\n",
    "        return a\n",
    "    \n",
    "\n",
    "    def perturb_actor_parameters(self, param_noise):\n",
    "        \"\"\"Apply parameter noise to actor model, for exploration\"\"\"\n",
    "        hard_update(self.actor_perturbed, self.actor)\n",
    "        params = self.actor_perturbed.state_dict()\n",
    "        for name in params:\n",
    "            if 'ln' in name: \n",
    "                pass \n",
    "            param = params[name]\n",
    "            random = torch.randn(param.shape)\n",
    "            if use_cuda:\n",
    "                random = random.cuda()\n",
    "            param += random * param_noise.current_stddev\n",
    "    \n",
    "    \n",
    "    def train(self,training_data):\n",
    "        # sample from Replay\n",
    "        batch_s = np.vstack(training_data[:,0])\n",
    "        batch_a = np.vstack(training_data[:,1])\n",
    "        batch_s1 = np.vstack(training_data[:,4])\n",
    "        batch_r = np.array(training_data[:,2]).astype(\"float\")\n",
    "        batch_done = np.array(training_data[:,3]).astype(\"float\")\n",
    "        \n",
    "        s1 = Variable(torch.from_numpy(batch_s)).type(FloatTensor)\n",
    "        a1 = Variable(torch.from_numpy(batch_a)).type(FloatTensor)\n",
    "        r1 = Variable(torch.from_numpy(batch_r[:,None]),volatile = True).type(FloatTensor)\n",
    "        s2 = Variable(torch.from_numpy(batch_s1), volatile = True).type(FloatTensor)\n",
    "        d  = Variable(torch.from_numpy(1.0*batch_done)[:,None]).type(FloatTensor)\n",
    "\n",
    "       \n",
    "        a2 = self.actor_target.forward(s2)\n",
    "        # ---------------------- optimize critic ----------------------\n",
    "\n",
    "        next_val = self.critic_target((s2, a2)).detach()\n",
    "        \n",
    "        q_expected = r1 + self.gamma*next_val*(1.0-d)\n",
    "        \n",
    "        # y_pred = Q( s1, a1)\n",
    "        q_predicted = self.critic((s1, a1))\n",
    "\n",
    "        \n",
    "        #print (y_predicted.volatile)\n",
    "        #print (y_expected.volatile)\n",
    "\n",
    "        # compute critic loss, and update the critic\n",
    "        loss_critic = self.critic_loss(q_predicted, q_expected)\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        #print (loss_critic.volatile)\n",
    "        loss_critic.backward()\n",
    "        self.optimizer_critic.step()\n",
    "\n",
    "        # ---------------------- optimize actor ----------------------\n",
    "        pred_a1 = self.actor.forward(s1)\n",
    "        loss_actor = -1*self.critic.forward((s1, pred_a1))\n",
    "        loss_actor = loss_actor.mean()\n",
    "        #input()\n",
    "        self.optimizer_actor.zero_grad()\n",
    "        loss_actor.backward()\n",
    "        #print (loss_actor.volatile)\n",
    "        self.optimizer_actor.step()\n",
    "        \n",
    "        # sychronize target network with fast moving one\n",
    "        weightSync(self.critic_target, self.critic)\n",
    "        weightSync(self.actor_target, self.actor)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of your DDPG object\n",
    "- Print network architectures, confirm they are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor(\n",
      "  (fc1): Linear(in_features=4, out_features=400, bias=True)\n",
      "  (fc2): Linear(in_features=400, out_features=400, bias=True)\n",
      "  (fc3): Linear(in_features=400, out_features=1, bias=True)\n",
      ")\n",
      "critic(\n",
      "  (fc1): Linear(in_features=4, out_features=300, bias=True)\n",
      "  (fc2): Linear(in_features=301, out_features=300, bias=True)\n",
      "  (fc3): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ddpg = DDPG(obs_dim = obs_dim, act_dim = act_dim)\n",
    "print(ddpg.actor)\n",
    "print(ddpg.critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DDPG on different environments\n",
    "Early stopping conditions:\n",
    "- avg_val > 500 for \"InvertedPendulum\" \n",
    "- avg_val > -150 for \"Pendulum\" \n",
    "- avg_val > 1500 for \"HalfCheetah\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveParamNoiseSpec(object):\n",
    "    def __init__(self, initial_stddev=0.1, desired_action_stddev=0.2, adaptation_coefficient=1.01):\n",
    "        \"\"\"\n",
    "        Note that initial_stddev and current_stddev refer to std of parameter noise, \n",
    "        but desired_action_stddev refers to (as name notes) desired std in action space\n",
    "        \"\"\"\n",
    "        self.initial_stddev = initial_stddev\n",
    "        self.desired_action_stddev = desired_action_stddev\n",
    "        self.adaptation_coefficient = adaptation_coefficient\n",
    "\n",
    "        self.current_stddev = initial_stddev\n",
    "\n",
    "    def adapt(self, distance):\n",
    "        if distance > self.desired_action_stddev:\n",
    "            # Decrease stddev.\n",
    "            self.current_stddev /= self.adaptation_coefficient\n",
    "        else:\n",
    "            # Increase stddev.\n",
    "            self.current_stddev *= self.adaptation_coefficient\n",
    "\n",
    "    def get_stats(self):\n",
    "        stats = {\n",
    "            'param_noise_stddev': self.current_stddev,\n",
    "        }\n",
    "        return stats\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt = 'AdaptiveParamNoiseSpec(initial_stddev={}, desired_action_stddev={}, adaptation_coefficient={})'\n",
    "        return fmt.format(self.initial_stddev, self.desired_action_stddev, self.adaptation_coefficient)\n",
    "\n",
    "def ddpg_distance_metric(actions1, actions2):\n",
    "    \"\"\"\n",
    "    Compute \"distance\" between actions taken by two policies at the same states\n",
    "    Expects numpy arrays\n",
    "    \"\"\"\n",
    "    diff = actions1-actions2\n",
    "    mean_diff = np.mean(np.square(diff), axis=0)\n",
    "    dist = sqrt(np.mean(mean_diff))\n",
    "    return dist\n",
    "\n",
    "\n",
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "           target_param.data.copy_(param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:78: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:79: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value: 0.8500000000000001 for episode: 0\n",
      "Average value: 1.2075 for episode: 1\n",
      "Average value: 1.2971249999999999 for episode: 2\n",
      "Average value: 1.3822687499999997 for episode: 3\n",
      "Average value: 1.4631553124999996 for episode: 4\n",
      "Average value: 1.5899975468749996 for episode: 5\n",
      "Average value: 1.6604976695312494 for episode: 6\n",
      "Average value: 1.7274727860546868 for episode: 7\n",
      "Average value: 1.7910991467519524 for episode: 8\n",
      "Average value: 1.8515441894143545 for episode: 9\n",
      "Average value: 1.9089669799436368 for episode: 10\n",
      "Average value: 1.9635186309464547 for episode: 11\n",
      "Average value: 2.015342699399132 for episode: 12\n",
      "Average value: 2.0645755644291754 for episode: 13\n",
      "Average value: 2.1113467862077164 for episode: 14\n",
      "Average value: 2.1557794468973306 for episode: 15\n",
      "Average value: 2.197990474552464 for episode: 16\n",
      "Average value: 2.2380909508248403 for episode: 17\n",
      "Average value: 2.276186403283598 for episode: 18\n",
      "Average value: 2.312377083119418 for episode: 19\n",
      "Average value: 2.346758228963447 for episode: 20\n",
      "Average value: 2.3794203175152746 for episode: 21\n",
      "Average value: 2.4104493016395105 for episode: 22\n",
      "Average value: 2.4399268365575346 for episode: 23\n",
      "Average value: 2.4679304947296576 for episode: 24\n",
      "Average value: 2.494533969993175 for episode: 25\n",
      "Average value: 2.519807271493516 for episode: 26\n",
      "Average value: 2.5438169079188397 for episode: 27\n",
      "Average value: 2.5666260625228974 for episode: 28\n",
      "Average value: 2.5882947593967525 for episode: 29\n",
      "Average value: 2.5588800214269147 for episode: 30\n",
      "Average value: 2.9309360203555688 for episode: 31\n",
      "Average value: 2.93438921933779 for episode: 32\n",
      "Average value: 2.9376697583709004 for episode: 33\n",
      "Average value: 2.9407862704523553 for episode: 34\n",
      "Average value: 3.8937469569297374 for episode: 35\n",
      "Average value: 3.84905960908325 for episode: 36\n",
      "Average value: 3.7566066286290876 for episode: 37\n",
      "Average value: 3.668776297197633 for episode: 38\n",
      "Average value: 3.585337482337751 for episode: 39\n",
      "Average value: 3.5060706082208632 for episode: 40\n",
      "Average value: 3.4807670778098196 for episode: 41\n",
      "Average value: 3.7567287239193288 for episode: 42\n",
      "Average value: 3.718892287723362 for episode: 43\n",
      "Average value: 3.6829476733371935 for episode: 44\n",
      "Average value: 3.6488002896703335 for episode: 45\n",
      "Average value: 3.6163602751868167 for episode: 46\n",
      "Average value: 3.5855422614274755 for episode: 47\n",
      "Average value: 3.5562651483561014 for episode: 48\n",
      "Average value: 3.528451890938296 for episode: 49\n",
      "Average value: 3.602029296391381 for episode: 50\n",
      "Average value: 3.521927831571812 for episode: 51\n",
      "Average value: 3.4958314399932213 for episode: 52\n",
      "Average value: 3.42103986799356 for episode: 53\n",
      "Average value: 3.3999878745938816 for episode: 54\n",
      "Average value: 3.3299884808641873 for episode: 55\n",
      "Average value: 3.313489056820978 for episode: 56\n",
      "Average value: 3.2478146039799287 for episode: 57\n",
      "Average value: 3.2854238737809323 for episode: 58\n",
      "Average value: 3.2711526800918853 for episode: 59\n",
      "Average value: 3.307595046087291 for episode: 60\n",
      "Average value: 3.442215293782926 for episode: 61\n",
      "Average value: 3.3701045290937794 for episode: 62\n",
      "Average value: 3.3515993026390904 for episode: 63\n",
      "Average value: 3.384019337507136 for episode: 64\n",
      "Average value: 3.3648183706317787 for episode: 65\n",
      "Average value: 3.3465774521001896 for episode: 66\n",
      "Average value: 3.32924857949518 for episode: 67\n",
      "Average value: 3.262786150520421 for episode: 68\n",
      "Average value: 3.1996468429944 for episode: 69\n",
      "Average value: 3.18966450084468 for episode: 70\n",
      "Average value: 3.280181275802446 for episode: 71\n",
      "Average value: 3.3661722120123234 for episode: 72\n",
      "Average value: 3.347863601411707 for episode: 73\n",
      "Average value: 3.2804704213411213 for episode: 74\n",
      "Average value: 3.266446900274065 for episode: 75\n",
      "Average value: 3.3531245552603615 for episode: 76\n",
      "Average value: 3.5354683274973433 for episode: 77\n",
      "Average value: 3.508694911122476 for episode: 78\n",
      "Average value: 3.4832601655663518 for episode: 79\n",
      "Average value: 3.5090971572880343 for episode: 80\n",
      "Average value: 3.5336422994236325 for episode: 81\n",
      "Average value: 3.5069601844524505 for episode: 82\n",
      "Average value: 3.431612175229828 for episode: 83\n",
      "Average value: 3.4100315664683363 for episode: 84\n",
      "Average value: 3.3395299881449194 for episode: 85\n",
      "Average value: 3.2725534887376733 for episode: 86\n",
      "Average value: 3.2089258143007897 for episode: 87\n",
      "Average value: 3.1484795235857503 for episode: 88\n",
      "Average value: 3.0910555474064627 for episode: 89\n",
      "Average value: 3.086502770036139 for episode: 90\n",
      "Average value: 3.1321776315343324 for episode: 91\n",
      "Average value: 3.1255687499576155 for episode: 92\n",
      "Average value: 3.1692903124597347 for episode: 93\n",
      "Average value: 3.210825796836748 for episode: 94\n",
      "Average value: 3.2002845069949104 for episode: 95\n",
      "Average value: 3.1402702816451646 for episode: 96\n",
      "Average value: 3.1332567675629064 for episode: 97\n",
      "Average value: 3.226593929184761 for episode: 98\n",
      "Average value: 3.2152642327255228 for episode: 99\n",
      "Average value: 3.2045010210892464 for episode: 100\n",
      "Average value: 3.144275970034784 for episode: 101\n",
      "Average value: 3.0870621715330446 for episode: 102\n",
      "Average value: 3.082709062956392 for episode: 103\n",
      "Average value: 3.0285736098085723 for episode: 104\n",
      "Average value: 2.977144929318144 for episode: 105\n",
      "Average value: 2.9782876828522364 for episode: 106\n",
      "Average value: 2.9793732987096244 for episode: 107\n",
      "Average value: 2.980404633774143 for episode: 108\n",
      "Average value: 2.9813844020854354 for episode: 109\n",
      "Average value: 3.0823151819811634 for episode: 110\n",
      "Average value: 3.028199422882105 for episode: 111\n",
      "Average value: 3.026789451738 for episode: 112\n",
      "Average value: 3.0254499791511 for episode: 113\n",
      "Average value: 2.9741774801935446 for episode: 114\n",
      "Average value: 2.9254686061838675 for episode: 115\n",
      "Average value: 2.929195175874674 for episode: 116\n",
      "Average value: 2.9827354170809404 for episode: 117\n",
      "Average value: 2.983598646226893 for episode: 118\n",
      "Average value: 2.9344187139155484 for episode: 119\n",
      "Average value: 2.887697778219771 for episode: 120\n",
      "Average value: 2.993312889308782 for episode: 121\n",
      "Average value: 2.943647244843343 for episode: 122\n",
      "Average value: 2.8964648826011756 for episode: 123\n",
      "Average value: 2.951641638471117 for episode: 124\n",
      "Average value: 2.954059556547561 for episode: 125\n",
      "Average value: 2.956356578720183 for episode: 126\n",
      "Average value: 3.0585387497841734 for episode: 127\n",
      "Average value: 3.0056118122949647 for episode: 128\n",
      "Average value: 3.1053312216802165 for episode: 129\n",
      "Average value: 3.1000646605962054 for episode: 130\n",
      "Average value: 3.1450614275663953 for episode: 131\n",
      "Average value: 3.137808356188075 for episode: 132\n",
      "Average value: 3.1309179383786714 for episode: 133\n",
      "Average value: 3.3743720414597376 for episode: 134\n",
      "Average value: 3.3556534393867503 for episode: 135\n",
      "Average value: 3.3378707674174124 for episode: 136\n",
      "Average value: 3.3209772290465414 for episode: 137\n",
      "Average value: 3.2549283675942142 for episode: 138\n",
      "Average value: 3.2421819492145034 for episode: 139\n",
      "Average value: 3.180072851753778 for episode: 140\n",
      "Average value: 3.371069209166089 for episode: 141\n",
      "Average value: 3.3025157487077847 for episode: 142\n",
      "Average value: 3.2373899612723953 for episode: 143\n",
      "Average value: 3.2255204632087753 for episode: 144\n",
      "Average value: 3.3142444400483364 for episode: 145\n",
      "Average value: 3.2485322180459195 for episode: 146\n",
      "Average value: 3.236105607143623 for episode: 147\n",
      "Average value: 3.224300326786442 for episode: 148\n",
      "Average value: 3.4130853104471197 for episode: 149\n",
      "Average value: 3.3924310449247637 for episode: 150\n",
      "Average value: 3.3228094926785254 for episode: 151\n",
      "Average value: 3.306669018044599 for episode: 152\n",
      "Average value: 3.391335567142369 for episode: 153\n",
      "Average value: 3.3217687887852505 for episode: 154\n",
      "Average value: 3.355680349345988 for episode: 155\n",
      "Average value: 3.2878963318786885 for episode: 156\n",
      "Average value: 3.373501515284754 for episode: 157\n",
      "Average value: 3.354826439520516 for episode: 158\n",
      "Average value: 3.33708511754449 for episode: 159\n",
      "Average value: 3.370230861667266 for episode: 160\n",
      "Average value: 3.3017193185839027 for episode: 161\n",
      "Average value: 3.2366333526547075 for episode: 162\n",
      "Average value: 3.274801685021972 for episode: 163\n",
      "Average value: 3.3610616007708733 for episode: 164\n",
      "Average value: 3.5430085207323296 for episode: 165\n",
      "Average value: 3.465858094695713 for episode: 166\n",
      "Average value: 3.3925651899609273 for episode: 167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value: 3.572936930462881 for episode: 168\n",
      "Average value: 3.494290083939737 for episode: 169\n",
      "Average value: 3.5695755797427497 for episode: 170\n",
      "Average value: 3.4910968007556122 for episode: 171\n",
      "Average value: 3.4665419607178314 for episode: 172\n",
      "Average value: 3.4432148626819394 for episode: 173\n",
      "Average value: 3.421054119547842 for episode: 174\n",
      "Average value: 3.35000141357045 for episode: 175\n",
      "Average value: 3.4325013428919275 for episode: 176\n",
      "Average value: 3.360876275747331 for episode: 177\n",
      "Average value: 3.442832461959964 for episode: 178\n",
      "Average value: 3.470690838861966 for episode: 179\n",
      "Average value: 3.4471562969188674 for episode: 180\n",
      "Average value: 3.524798482072924 for episode: 181\n",
      "Average value: 3.4985585579692775 for episode: 182\n",
      "Average value: 3.4736306300708133 for episode: 183\n",
      "Average value: 3.4499490985672723 for episode: 184\n",
      "Average value: 3.4274516436389084 for episode: 185\n",
      "Average value: 3.4060790614569627 for episode: 186\n",
      "Average value: 3.4857751083841144 for episode: 187\n",
      "Average value: 3.4114863529649084 for episode: 188\n",
      "Average value: 3.340912035316663 for episode: 189\n",
      "Average value: 3.3238664335508292 for episode: 190\n",
      "Average value: 3.8076731118732874 for episode: 191\n",
      "Average value: 3.767289456279623 for episode: 192\n",
      "Average value: 3.7789249834656418 for episode: 193\n",
      "Average value: 3.7399787342923596 for episode: 194\n",
      "Average value: 4.452979797577742 for episode: 195\n",
      "Average value: 4.480330807698854 for episode: 196\n",
      "Average value: 4.356314267313911 for episode: 197\n",
      "Average value: 4.238498553948214 for episode: 198\n",
      "Average value: 4.1765736262508035 for episode: 199\n",
      "Average value: 4.117744944938264 for episode: 200\n",
      "Average value: 4.16185769769135 for episode: 201\n",
      "Average value: 4.103764812806783 for episode: 202\n",
      "Average value: 3.9985765721664435 for episode: 203\n",
      "Average value: 3.8986477435581213 for episode: 204\n",
      "Average value: 4.103715356380215 for episode: 205\n",
      "Average value: 4.0485295885612045 for episode: 206\n",
      "Average value: 3.946103109133144 for episode: 207\n",
      "Average value: 3.8987979536764867 for episode: 208\n",
      "Average value: 3.9538580559926624 for episode: 209\n",
      "Average value: 3.856165153193029 for episode: 210\n",
      "Average value: 3.7633568955333776 for episode: 211\n",
      "Average value: 3.7751890507567087 for episode: 212\n",
      "Average value: 3.686429598218873 for episode: 213\n",
      "Average value: 3.6521081183079294 for episode: 214\n",
      "Average value: 3.719502712392533 for episode: 215\n",
      "Average value: 3.6335275767729063 for episode: 216\n",
      "Average value: 3.551851197934261 for episode: 217\n",
      "Average value: 3.474258638037548 for episode: 218\n",
      "Average value: 3.4005457061356705 for episode: 219\n",
      "Average value: 3.330518420828887 for episode: 220\n",
      "Average value: 3.2639924997874425 for episode: 221\n",
      "Average value: 3.3007928747980704 for episode: 222\n",
      "Average value: 3.6857532310581664 for episode: 223\n",
      "Average value: 3.651465569505258 for episode: 224\n",
      "Average value: 3.568892291029995 for episode: 225\n",
      "Average value: 3.4904476764784955 for episode: 226\n",
      "Average value: 3.765925292654571 for episode: 227\n",
      "Average value: 3.677629028021842 for episode: 228\n",
      "Average value: 3.6437475766207497 for episode: 229\n",
      "Average value: 3.611560197789712 for episode: 230\n",
      "Average value: 3.6309821879002264 for episode: 231\n",
      "Average value: 3.549433078505215 for episode: 232\n",
      "Average value: 3.521961424579954 for episode: 233\n",
      "Average value: 3.495863353350956 for episode: 234\n",
      "Average value: 3.471070185683408 for episode: 235\n",
      "Average value: 3.3975166763992375 for episode: 236\n",
      "Average value: 3.3276408425792754 for episode: 237\n",
      "Average value: 3.3112588004503114 for episode: 238\n",
      "Average value: 3.2956958604277955 for episode: 239\n",
      "Average value: 3.2809110674064055 for episode: 240\n",
      "Average value: 3.266865514036085 for episode: 241\n",
      "Average value: 3.2035222383342807 for episode: 242\n",
      "Average value: 3.143346126417567 for episode: 243\n",
      "Average value: 3.1361788200966885 for episode: 244\n",
      "Average value: 3.079369879091854 for episode: 245\n",
      "Average value: 3.075401385137261 for episode: 246\n",
      "Average value: 3.071631315880398 for episode: 247\n",
      "Average value: 3.0680497500863777 for episode: 248\n",
      "Average value: 3.014647262582059 for episode: 249\n",
      "Average value: 2.963914899452956 for episode: 250\n",
      "Average value: 2.965719154480308 for episode: 251\n",
      "Average value: 2.9174331967562925 for episode: 252\n",
      "Average value: 2.971561536918478 for episode: 253\n",
      "Average value: 2.922983460072554 for episode: 254\n",
      "Average value: 2.9768342870689266 for episode: 255\n",
      "Average value: 2.97799257271548 for episode: 256\n",
      "Average value: 3.0790929440797057 for episode: 257\n",
      "Average value: 3.0251382968757206 for episode: 258\n",
      "Average value: 3.0238813820319344 for episode: 259\n",
      "Average value: 3.072687312930338 for episode: 260\n",
      "Average value: 3.069052947283821 for episode: 261\n",
      "Average value: 3.0656002999196295 for episode: 262\n",
      "Average value: 3.012320284923648 for episode: 263\n",
      "Average value: 2.9617042706774654 for episode: 264\n",
      "Average value: 2.963619057143592 for episode: 265\n",
      "Average value: 2.9154381042864124 for episode: 266\n",
      "Average value: 2.9196661990720916 for episode: 267\n",
      "Average value: 2.873682889118487 for episode: 268\n",
      "Average value: 2.8799987446625623 for episode: 269\n",
      "Average value: 2.835998807429434 for episode: 270\n",
      "Average value: 2.794198867057962 for episode: 271\n",
      "Average value: 2.9044889237050637 for episode: 272\n",
      "Average value: 3.0092644775198103 for episode: 273\n",
      "Average value: 3.0088012536438193 for episode: 274\n",
      "Average value: 3.008361190961628 for episode: 275\n",
      "Average value: 3.157943131413546 for episode: 276\n",
      "Average value: 3.1500459748428686 for episode: 277\n",
      "Average value: 3.142543676100725 for episode: 278\n",
      "Average value: 3.0854164922956886 for episode: 279\n",
      "Average value: 3.0311456676809043 for episode: 280\n",
      "Average value: 2.979588384296859 for episode: 281\n",
      "Average value: 2.930608965082016 for episode: 282\n",
      "Average value: 2.8840785168279153 for episode: 283\n",
      "Average value: 2.889874590986519 for episode: 284\n",
      "Average value: 2.895380861437193 for episode: 285\n",
      "Average value: 2.9006118183653333 for episode: 286\n",
      "Average value: 3.0555812274470666 for episode: 287\n",
      "Average value: 3.002802166074713 for episode: 288\n",
      "Average value: 3.0026620577709773 for episode: 289\n",
      "Average value: 2.9525289548824283 for episode: 290\n",
      "Average value: 2.9549025071383066 for episode: 291\n",
      "Average value: 2.957157381781391 for episode: 292\n",
      "Average value: 3.0092995126923214 for episode: 293\n",
      "Average value: 3.008834537057705 for episode: 294\n",
      "Average value: 3.00839281020482 for episode: 295\n",
      "Average value: 3.0079731696945786 for episode: 296\n",
      "Average value: 3.4575745112098497 for episode: 297\n",
      "Average value: 3.434695785649357 for episode: 298\n",
      "Average value: 3.362960996366889 for episode: 299\n",
      "Average value: 3.2948129465485447 for episode: 300\n",
      "Average value: 3.2300722992211175 for episode: 301\n",
      "Average value: 3.1685686842600616 for episode: 302\n",
      "Average value: 3.1601402500470583 for episode: 303\n",
      "Average value: 3.1521332375447053 for episode: 304\n",
      "Average value: 3.09452657566747 for episode: 305\n",
      "Average value: 3.239800246884096 for episode: 306\n",
      "Average value: 3.577810234539891 for episode: 307\n",
      "Average value: 3.8489197228128966 for episode: 308\n",
      "Average value: 3.9064737366722517 for episode: 309\n",
      "Average value: 3.861150049838639 for episode: 310\n",
      "Average value: 3.9180925473467068 for episode: 311\n",
      "Average value: 3.872187919979371 for episode: 312\n",
      "Average value: 3.8285785239804024 for episode: 313\n",
      "Average value: 4.287149597781382 for episode: 314\n",
      "Average value: 7.972792117892313 for episode: 315\n",
      "Average value: 7.724152511997698 for episode: 316\n",
      "Average value: 7.637944886397812 for episode: 317\n",
      "Average value: 7.506047642077921 for episode: 318\n",
      "Average value: 8.780745259974024 for episode: 319\n",
      "Average value: 8.491707996975324 for episode: 320\n",
      "Average value: 8.717122597126558 for episode: 321\n",
      "Average value: 10.231266467270231 for episode: 322\n",
      "Average value: 10.069703143906718 for episode: 323\n",
      "Average value: 17.46621798671138 for episode: 324\n",
      "Average value: 17.09290708737581 for episode: 325\n",
      "Average value: 16.53826173300702 for episode: 326\n",
      "Average value: 16.46134864635667 for episode: 327\n",
      "Average value: 15.988281214038834 for episode: 328\n",
      "Average value: 15.538867153336891 for episode: 329\n",
      "Average value: 15.711923795670046 for episode: 330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value: 15.276327605886543 for episode: 331\n",
      "Average value: 15.012511225592215 for episode: 332\n",
      "Average value: 14.661885664312605 for episode: 333\n",
      "Average value: 14.278791381096973 for episode: 334\n",
      "Average value: 16.56485181204212 for episode: 335\n",
      "Average value: 18.036609221440013 for episode: 336\n",
      "Average value: 17.484778760368012 for episode: 337\n",
      "Average value: 19.36053982234961 for episode: 338\n",
      "Average value: 19.242512831232133 for episode: 339\n",
      "Average value: 18.930387189670526 for episode: 340\n",
      "Average value: 18.633867830186997 for episode: 341\n",
      "Average value: 18.602174438677643 for episode: 342\n",
      "Average value: 18.572065716743758 for episode: 343\n",
      "Average value: 18.54346243090657 for episode: 344\n",
      "Average value: 19.016289309361238 for episode: 345\n",
      "Average value: 19.265474843893173 for episode: 346\n",
      "Average value: 19.852201101698515 for episode: 347\n",
      "Average value: 19.259591046613586 for episode: 348\n",
      "Average value: 18.596611494282907 for episode: 349\n",
      "Average value: 19.36678091956876 for episode: 350\n",
      "Average value: 18.648441873590322 for episode: 351\n",
      "Average value: 21.666019779910805 for episode: 352\n",
      "Average value: 20.882718790915263 for episode: 353\n",
      "Average value: 21.538582851369497 for episode: 354\n",
      "Average value: 21.461653708801023 for episode: 355\n",
      "Average value: 21.738571023360972 for episode: 356\n",
      "Average value: 21.80164247219292 for episode: 357\n",
      "Average value: 21.161560348583272 for episode: 358\n",
      "Average value: 23.90348233115411 for episode: 359\n",
      "Average value: 23.058308214596405 for episode: 360\n",
      "Average value: 23.855392803866582 for episode: 361\n",
      "Average value: 24.762623163673254 for episode: 362\n",
      "Average value: 34.42449200548959 for episode: 363\n",
      "Average value: 34.753267405215105 for episode: 364\n",
      "Average value: 36.01560403495435 for episode: 365\n",
      "Average value: 38.11482383320663 for episode: 366\n",
      "Average value: 37.4090826415463 for episode: 367\n",
      "Average value: 37.73862850946899 for episode: 368\n",
      "Average value: 44.35169708399554 for episode: 369\n",
      "Average value: 44.584112229795764 for episode: 370\n",
      "Average value: 46.354906618305975 for episode: 371\n",
      "Average value: 46.93716128739067 for episode: 372\n",
      "Average value: 51.04030322302114 for episode: 373\n",
      "Average value: 50.73828806187008 for episode: 374\n",
      "Average value: 50.65137365877658 for episode: 375\n",
      "Average value: 52.168804975837745 for episode: 376\n",
      "Average value: 53.610364727045855 for episode: 377\n",
      "Average value: 52.779846490693565 for episode: 378\n",
      "Average value: 54.590854166158884 for episode: 379\n",
      "Average value: 55.661311457850935 for episode: 380\n",
      "Average value: 56.32824588495839 for episode: 381\n",
      "Average value: 55.76183359071047 for episode: 382\n",
      "Average value: 56.12374191117494 for episode: 383\n",
      "Average value: 57.517554815616194 for episode: 384\n",
      "Average value: 63.49167707483538 for episode: 385\n",
      "Average value: 61.367093221093604 for episode: 386\n",
      "Average value: 61.54873856003892 for episode: 387\n",
      "Average value: 62.82130163203697 for episode: 388\n",
      "Average value: 70.93023655043513 for episode: 389\n",
      "Average value: 70.13372472291337 for episode: 390\n",
      "Average value: 85.5770384867677 for episode: 391\n",
      "Average value: 83.69818656242933 for episode: 392\n",
      "Average value: 84.01327723430786 for episode: 393\n",
      "Average value: 129.81261337259247 for episode: 394\n",
      "Average value: 127.52198270396283 for episode: 395\n",
      "Average value: 125.09588356876469 for episode: 396\n",
      "Average value: 137.59108939032643 for episode: 397\n",
      "Average value: 136.66153492081008 for episode: 398\n",
      "Average value: 144.97845817476957 for episode: 399\n",
      "Average value: 141.0795352660311 for episode: 400\n",
      "Average value: 136.27555850272952 for episode: 401\n",
      "Average value: 133.61178057759304 for episode: 402\n",
      "Average value: 127.38119154871339 for episode: 403\n",
      "Average value: 124.8621319712777 for episode: 404\n",
      "Average value: 121.61902537271382 for episode: 405\n",
      "Average value: 119.23807410407812 for episode: 406\n",
      "Average value: 115.97617039887422 for episode: 407\n",
      "Average value: 115.9773618789305 for episode: 408\n",
      "Average value: 113.87849378498397 for episode: 409\n",
      "Average value: 113.68456909573477 for episode: 410\n",
      "Average value: 118.15034064094803 for episode: 411\n",
      "Average value: 115.24282360890062 for episode: 412\n",
      "Average value: 117.43068242845558 for episode: 413\n",
      "Average value: 115.8591483070328 for episode: 414\n",
      "Average value: 140.21619089168115 for episode: 415\n",
      "Average value: 137.50538134709709 for episode: 416\n",
      "Average value: 133.93011227974225 for episode: 417\n",
      "Average value: 133.83360666575513 for episode: 418\n",
      "Average value: 131.49192633246736 for episode: 419\n",
      "Average value: 127.51733001584398 for episode: 420\n",
      "Average value: 123.04146351505179 for episode: 421\n",
      "Average value: 133.8893903392992 for episode: 422\n",
      "Average value: 132.64492082233423 for episode: 423\n",
      "Average value: 128.5626747812175 for episode: 424\n",
      "Average value: 136.13454104215663 for episode: 425\n",
      "Average value: 132.6278139900488 for episode: 426\n",
      "Average value: 131.89642329054635 for episode: 427\n",
      "Average value: 131.75160212601904 for episode: 428\n",
      "Average value: 128.21402201971807 for episode: 429\n",
      "Average value: 125.20332091873217 for episode: 430\n",
      "Average value: 124.89315487279556 for episode: 431\n",
      "Average value: 121.14849712915579 for episode: 432\n",
      "Average value: 118.14107227269798 for episode: 433\n",
      "Average value: 117.78401865906308 for episode: 434\n",
      "Average value: 114.74481772610991 for episode: 435\n",
      "Average value: 112.85757683980441 for episode: 436\n",
      "Average value: 111.11469799781419 for episode: 437\n",
      "Average value: 108.70896309792349 for episode: 438\n",
      "Average value: 106.67351494302731 for episode: 439\n",
      "Average value: 106.58983919587594 for episode: 440\n",
      "Average value: 103.76034723608214 for episode: 441\n",
      "Average value: 103.47232987427803 for episode: 442\n",
      "Average value: 105.69871338056413 for episode: 443\n",
      "Average value: 104.16377771153591 for episode: 444\n",
      "Average value: 101.85558882595912 for episode: 445\n",
      "Average value: 99.41280938466116 for episode: 446\n",
      "Average value: 104.5921689154281 for episode: 447\n",
      "Average value: 106.7125604696567 for episode: 448\n",
      "Average value: 103.97693244617385 for episode: 449\n",
      "Average value: 102.62808582386515 for episode: 450\n",
      "Average value: 102.09668153267188 for episode: 451\n",
      "Average value: 100.94184745603829 for episode: 452\n",
      "Average value: 110.74475508323638 for episode: 453\n",
      "Average value: 111.25751732907456 for episode: 454\n",
      "Average value: 110.49464146262082 for episode: 455\n",
      "Average value: 115.91990938948977 for episode: 456\n",
      "Average value: 112.17391392001528 for episode: 457\n",
      "Average value: 114.96521822401452 for episode: 458\n",
      "Average value: 113.26695731281379 for episode: 459\n",
      "Average value: 113.3536094471731 for episode: 460\n",
      "Average value: 110.98592897481444 for episode: 461\n",
      "Average value: 109.7866325260737 for episode: 462\n",
      "Average value: 107.64730089977 for episode: 463\n",
      "Average value: 106.61493585478149 for episode: 464\n",
      "Average value: 106.68418906204242 for episode: 465\n",
      "Average value: 104.0999796089403 for episode: 466\n",
      "Average value: 101.89498062849329 for episode: 467\n",
      "Average value: 99.20023159706862 for episode: 468\n",
      "Average value: 99.1402200172152 for episode: 469\n",
      "Average value: 99.53320901635443 for episode: 470\n",
      "Average value: 99.5065485655367 for episode: 471\n",
      "Average value: 117.68122113725987 for episode: 472\n",
      "Average value: 118.49716008039687 for episode: 473\n",
      "Average value: 119.17230207637701 for episode: 474\n",
      "Average value: 116.51368697255815 for episode: 475\n",
      "Average value: 118.13800262393023 for episode: 476\n",
      "Average value: 121.63110249273372 for episode: 477\n",
      "Average value: 121.89954736809702 for episode: 478\n",
      "Average value: 118.20456999969217 for episode: 479\n",
      "Average value: 115.54434149970756 for episode: 480\n",
      "Average value: 115.71712442472217 for episode: 481\n",
      "Average value: 116.58126820348606 for episode: 482\n",
      "Average value: 113.05220479331174 for episode: 483\n",
      "Average value: 109.74959455364615 for episode: 484\n",
      "Average value: 106.86211482596383 for episode: 485\n",
      "Average value: 107.86900908466562 for episode: 486\n",
      "Average value: 108.37555863043234 for episode: 487\n",
      "Average value: 106.40678069891072 for episode: 488\n",
      "Average value: 104.98644166396518 for episode: 489\n",
      "Average value: 105.48711958076692 for episode: 490\n",
      "Average value: 104.96276360172857 for episode: 491\n",
      "Average value: 109.16462542164214 for episode: 492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value: 106.60639415056004 for episode: 493\n",
      "Average value: 105.87607444303202 for episode: 494\n",
      "Average value: 104.03227072088042 for episode: 495\n",
      "Average value: 101.0806571848364 for episode: 496\n",
      "Average value: 98.77662432559457 for episode: 497\n",
      "Average value: 95.93779310931482 for episode: 498\n",
      "Average value: 95.64090345384908 for episode: 499\n",
      "Average value: 99.05885828115662 for episode: 500\n",
      "Average value: 96.90591536709879 for episode: 501\n",
      "Average value: 94.56061959874384 for episode: 502\n",
      "Average value: 92.68258861880663 for episode: 503\n",
      "Average value: 92.8984591878663 for episode: 504\n",
      "Average value: 90.10353622847298 for episode: 505\n",
      "Average value: 91.19835941704932 for episode: 506\n",
      "Average value: 89.08844144619685 for episode: 507\n",
      "Average value: 87.584019373887 for episode: 508\n",
      "Average value: 86.65481840519266 for episode: 509\n",
      "Average value: 84.92207748493301 for episode: 510\n",
      "Average value: 83.22597361068635 for episode: 511\n",
      "Average value: 81.11467493015203 for episode: 512\n",
      "Average value: 79.20894118364444 for episode: 513\n",
      "Average value: 77.39849412446222 for episode: 514\n",
      "Average value: 75.5285694182391 for episode: 515\n",
      "Average value: 79.45214094732715 for episode: 516\n",
      "Average value: 78.22953389996078 for episode: 517\n",
      "Average value: 78.11805720496274 for episode: 518\n",
      "Average value: 78.4621543447146 for episode: 519\n",
      "Average value: 77.08904662747887 for episode: 520\n",
      "Average value: 80.88459429610492 for episode: 521\n",
      "Average value: 85.04036458129967 for episode: 522\n",
      "Average value: 87.58834635223468 for episode: 523\n",
      "Average value: 86.15892903462294 for episode: 524\n",
      "Average value: 87.0009825828918 for episode: 525\n",
      "Average value: 84.7009334537472 for episode: 526\n",
      "Average value: 83.51588678105983 for episode: 527\n",
      "Average value: 82.34009244200683 for episode: 528\n",
      "Average value: 88.0730878199065 for episode: 529\n",
      "Average value: 88.41943342891118 for episode: 530\n",
      "Average value: 88.29846175746562 for episode: 531\n",
      "Average value: 87.28353866959233 for episode: 532\n",
      "Average value: 87.1693617361127 for episode: 533\n",
      "Average value: 84.81089364930706 for episode: 534\n",
      "Average value: 86.4203489668417 for episode: 535\n",
      "Average value: 86.54933151849961 for episode: 536\n",
      "Average value: 88.42186494257463 for episode: 537\n",
      "Average value: 89.30077169544589 for episode: 538\n",
      "Average value: 89.73573311067359 for episode: 539\n",
      "Average value: 89.54894645513991 for episode: 540\n",
      "Average value: 88.6714991323829 for episode: 541\n",
      "Average value: 86.48792417576375 for episode: 542\n",
      "Average value: 91.66352796697556 for episode: 543\n",
      "Average value: 99.43035156862678 for episode: 544\n",
      "Average value: 101.30883399019542 for episode: 545\n",
      "Average value: 99.64339229068565 for episode: 546\n",
      "Average value: 102.46122267615137 for episode: 547\n",
      "Average value: 104.1381615423438 for episode: 548\n",
      "Average value: 106.1312534652266 for episode: 549\n",
      "Average value: 108.62469079196526 for episode: 550\n",
      "Average value: 109.193456252367 for episode: 551\n",
      "Average value: 107.68378343974865 for episode: 552\n",
      "Average value: 111.04959426776121 for episode: 553\n",
      "Average value: 110.09711455437314 for episode: 554\n",
      "Average value: 108.29225882665448 for episode: 555\n",
      "Average value: 109.17764588532174 for episode: 556\n",
      "Average value: 106.56876359105564 for episode: 557\n",
      "Average value: 105.19032541150285 for episode: 558\n",
      "Average value: 106.9308091409277 for episode: 559\n",
      "Average value: 111.38426868388132 for episode: 560\n",
      "Average value: 111.96505524968725 for episode: 561\n",
      "Average value: 108.86680248720289 for episode: 562\n",
      "Average value: 115.17346236284274 for episode: 563\n",
      "Average value: 111.6147892447006 for episode: 564\n",
      "Average value: 111.53404978246557 for episode: 565\n",
      "Average value: 111.2073472933423 for episode: 566\n",
      "Average value: 109.94697992867518 for episode: 567\n",
      "Average value: 110.79963093224141 for episode: 568\n",
      "Average value: 109.55964938562934 for episode: 569\n",
      "Average value: 110.23166691634788 for episode: 570\n",
      "Average value: 111.52008357053047 for episode: 571\n",
      "Average value: 116.19407939200394 for episode: 572\n",
      "Average value: 112.93437542240373 for episode: 573\n",
      "Average value: 111.58765665128354 for episode: 574\n",
      "Average value: 121.10827381871937 for episode: 575\n",
      "Average value: 124.25286012778339 for episode: 576\n",
      "Average value: 126.24021712139422 for episode: 577\n",
      "Average value: 133.8282062653245 for episode: 578\n",
      "Average value: 137.48679595205826 for episode: 579\n",
      "Average value: 135.41245615445536 for episode: 580\n",
      "Average value: 131.44183334673258 for episode: 581\n",
      "Average value: 131.61974167939593 for episode: 582\n",
      "Average value: 134.08875459542614 for episode: 583\n",
      "Average value: 165.53431686565483 for episode: 584\n",
      "Average value: 160.6576010223721 for episode: 585\n",
      "Average value: 162.72472097125348 for episode: 586\n",
      "Average value: 160.33848492269078 for episode: 587\n",
      "Average value: 157.57156067655623 for episode: 588\n",
      "Average value: 154.09298264272843 for episode: 589\n",
      "Average value: 150.788333510592 for episode: 590\n",
      "Average value: 146.5489168350624 for episode: 591\n",
      "Average value: 142.97147099330928 for episode: 592\n",
      "Average value: 138.22289744364383 for episode: 593\n",
      "Average value: 135.86175257146164 for episode: 594\n",
      "Average value: 134.46866494288855 for episode: 595\n",
      "Average value: 130.09523169574413 for episode: 596\n",
      "Average value: 126.94047011095691 for episode: 597\n",
      "Average value: 123.14344660540905 for episode: 598\n",
      "Average value: 120.4862742751386 for episode: 599\n",
      "Average value: 117.91196056138166 for episode: 600\n",
      "Average value: 115.96636253331258 for episode: 601\n",
      "Average value: 116.26804440664694 for episode: 602\n",
      "Average value: 121.90464218631459 for episode: 603\n",
      "Average value: 118.90941007699885 for episode: 604\n",
      "Average value: 116.41393957314891 for episode: 605\n",
      "Average value: 113.69324259449145 for episode: 606\n",
      "Average value: 119.50858046476688 for episode: 607\n",
      "Average value: 117.73315144152853 for episode: 608\n",
      "Average value: 116.5964938694521 for episode: 609\n",
      "Average value: 114.56666917597948 for episode: 610\n",
      "Average value: 114.6383357171805 for episode: 611\n",
      "Average value: 111.00641893132146 for episode: 612\n",
      "Average value: 108.55609798475538 for episode: 613\n",
      "Average value: 106.12829308551761 for episode: 614\n",
      "Average value: 103.92187843124172 for episode: 615\n",
      "Average value: 101.02578450967962 for episode: 616\n",
      "Average value: 98.97449528419564 for episode: 617\n",
      "Average value: 96.97577051998586 for episode: 618\n",
      "Average value: 99.17698199398656 for episode: 619\n",
      "Average value: 144.21813289428724 for episode: 620\n",
      "Average value: 140.45722624957287 for episode: 621\n",
      "Average value: 142.33436493709422 for episode: 622\n",
      "Average value: 139.31764669023948 for episode: 623\n",
      "Average value: 138.00176435572752 for episode: 624\n",
      "Average value: 135.40167613794114 for episode: 625\n",
      "Average value: 135.43159233104407 for episode: 626\n",
      "Average value: 134.51001271449186 for episode: 627\n",
      "Average value: 132.38451207876727 for episode: 628\n",
      "Average value: 130.8652864748289 for episode: 629\n",
      "Average value: 130.47202215108746 for episode: 630\n",
      "Average value: 127.64842104353308 for episode: 631\n",
      "Average value: 126.56599999135642 for episode: 632\n",
      "Average value: 124.23769999178859 for episode: 633\n",
      "Average value: 122.42581499219916 for episode: 634\n",
      "Average value: 121.0045242425892 for episode: 635\n",
      "Average value: 118.75429803045974 for episode: 636\n",
      "Average value: 117.16658312893674 for episode: 637\n",
      "Average value: 116.0582539724899 for episode: 638\n",
      "Average value: 113.2053412738654 for episode: 639\n",
      "Average value: 118.14507421017214 for episode: 640\n",
      "Average value: 117.08782049966352 for episode: 641\n",
      "Average value: 115.18342947468034 for episode: 642\n",
      "Average value: 124.17425800094631 for episode: 643\n",
      "Average value: 120.865545100899 for episode: 644\n",
      "Average value: 119.57226784585404 for episode: 645\n",
      "Average value: 119.39365445356134 for episode: 646\n",
      "Average value: 117.22397173088326 for episode: 647\n",
      "Average value: 114.3127731443391 for episode: 648\n",
      "Average value: 111.89713448712214 for episode: 649\n",
      "Average value: 117.25227776276604 for episode: 650\n",
      "Average value: 115.58966387462773 for episode: 651\n",
      "Average value: 128.06018068089634 for episode: 652\n",
      "Average value: 126.30717164685151 for episode: 653\n",
      "Average value: 125.39181306450894 for episode: 654\n",
      "Average value: 124.62222241128349 for episode: 655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value: 122.2411112907193 for episode: 656\n",
      "Average value: 124.27905572618334 for episode: 657\n",
      "Average value: 123.01510293987417 for episode: 658\n",
      "Average value: 123.51434779288046 for episode: 659\n",
      "Average value: 122.88863040323642 for episode: 660\n",
      "Average value: 124.1441988830746 for episode: 661\n",
      "Average value: 122.23698893892086 for episode: 662\n",
      "Average value: 121.1251394919748 for episode: 663\n",
      "Average value: 127.56888251737607 for episode: 664\n",
      "Average value: 124.04043839150725 for episode: 665\n",
      "Average value: 134.03841647193187 for episode: 666\n",
      "Average value: 136.23649564833528 for episode: 667\n",
      "Average value: 158.9246708659185 for episode: 668\n",
      "Average value: 153.72843732262257 for episode: 669\n",
      "Average value: 149.09201545649145 for episode: 670\n",
      "Average value: 146.23741468366686 for episode: 671\n",
      "Average value: 143.4255439494835 for episode: 672\n",
      "Average value: 143.80426675200934 for episode: 673\n",
      "Average value: 147.66405341440887 for episode: 674\n",
      "Average value: 146.08085074368844 for episode: 675\n",
      "Average value: 144.02680820650403 for episode: 676\n",
      "Average value: 144.57546779617883 for episode: 677\n",
      "Average value: 151.54669440636988 for episode: 678\n",
      "Average value: 147.46935968605138 for episode: 679\n",
      "Average value: 144.6958917017488 for episode: 680\n",
      "Average value: 150.56109711666133 for episode: 681\n",
      "Average value: 150.48304226082826 for episode: 682\n",
      "Average value: 147.85889014778684 for episode: 683\n",
      "Average value: 144.96594564039748 for episode: 684\n",
      "Average value: 141.0676483583776 for episode: 685\n",
      "Average value: 137.0642659404587 for episode: 686\n",
      "Average value: 134.11105264343578 for episode: 687\n",
      "Average value: 133.30550001126397 for episode: 688\n",
      "Average value: 131.84022501070078 for episode: 689\n",
      "Average value: 133.14821376016573 for episode: 690\n",
      "Average value: 130.99080307215743 for episode: 691\n",
      "Average value: 133.39126291854956 for episode: 692\n",
      "Average value: 162.57169977262208 for episode: 693\n",
      "Average value: 160.94311478399098 for episode: 694\n",
      "Average value: 188.74595904479142 for episode: 695\n",
      "Average value: 181.70866109255184 for episode: 696\n",
      "Average value: 177.17322803792425 for episode: 697\n",
      "Average value: 170.86456663602803 for episode: 698\n",
      "Average value: 165.47133830422663 for episode: 699\n",
      "Average value: 178.79777138901528 for episode: 700\n",
      "Average value: 174.75788281956451 for episode: 701\n",
      "Average value: 179.4199886785863 for episode: 702\n",
      "Average value: 174.64898924465695 for episode: 703\n",
      "Average value: 170.76653978242408 for episode: 704\n",
      "Average value: 212.22821279330287 for episode: 705\n",
      "Average value: 214.2668021536377 for episode: 706\n",
      "Average value: 207.40346204595582 for episode: 707\n",
      "Average value: 207.63328894365802 for episode: 708\n",
      "Average value: 247.2516244964751 for episode: 709\n",
      "Average value: 241.28904327165134 for episode: 710\n",
      "Average value: 279.22459110806875 for episode: 711\n",
      "Average value: 273.2133615526653 for episode: 712\n",
      "Average value: 266.202693475032 for episode: 713\n",
      "Average value: 262.2925588012804 for episode: 714\n",
      "Average value: 284.27793086121636 for episode: 715\n",
      "Average value: 272.8640343181555 for episode: 716\n",
      "Average value: 262.47083260224775 for episode: 717\n",
      "Average value: 251.54729097213533 for episode: 718\n",
      "Average value: 277.26992642352855 for episode: 719\n",
      "Average value: 269.80643010235207 for episode: 720\n",
      "Average value: 306.31610859723446 for episode: 721\n",
      "Average value: 294.50030316737275 for episode: 722\n",
      "Average value: 298.3252880090041 for episode: 723\n",
      "Average value: 333.40902360855387 for episode: 724\n",
      "Average value: 360.98857242812613 for episode: 725\n",
      "Average value: 346.4891438067198 for episode: 726\n",
      "Average value: 333.5146866163838 for episode: 727\n",
      "Average value: 328.33895228556463 for episode: 728\n",
      "Average value: 317.7220046712864 for episode: 729\n",
      "Average value: 305.2859044377221 for episode: 730\n",
      "Average value: 296.67160921583593 for episode: 731\n",
      "Average value: 293.48802875504407 for episode: 732\n",
      "Average value: 296.81362731729183 for episode: 733\n",
      "Average value: 331.9729459514272 for episode: 734\n",
      "Average value: 333.8242986538558 for episode: 735\n",
      "Average value: 324.033083721163 for episode: 736\n",
      "Average value: 310.6314295351048 for episode: 737\n",
      "Average value: 300.9498580583496 for episode: 738\n",
      "Average value: 310.2023651554321 for episode: 739\n",
      "Average value: 299.0422468976605 for episode: 740\n",
      "Average value: 334.09013455277744 for episode: 741\n",
      "Average value: 322.13562782513856 for episode: 742\n",
      "Average value: 314.9788464338816 for episode: 743\n",
      "Average value: 306.6799041121875 for episode: 744\n",
      "Average value: 299.4959089065781 for episode: 745\n",
      "Average value: 308.72111346124916 for episode: 746\n",
      "Average value: 300.68505778818667 for episode: 747\n",
      "Average value: 335.6508048987773 for episode: 748\n",
      "Average value: 368.8682646538384 for episode: 749\n",
      "Average value: 361.9748514211465 for episode: 750\n",
      "Average value: 393.87610885008917 for episode: 751\n",
      "Average value: 380.3323034075847 for episode: 752\n",
      "Average value: 411.31568823720545 for episode: 753\n",
      "Average value: 440.74990382534514 for episode: 754\n",
      "Average value: 433.86240863407784 for episode: 755\n",
      "Average value: 462.16928820237393 for episode: 756\n",
      "Average value: 489.06082379225523 for episode: 757\n",
      "Average value: 514.6077826026424 for episode: 758\n",
      "Average value: 538.8773934725102 for episode: 759\n",
      "Average value: 561.9335237988846 for episode: 760\n",
      "Average value: 583.8368476089404 for episode: 761\n",
      "Average value: 604.6450052284933 for episode: 762\n",
      "Average value: 624.4127549670686 for episode: 763\n",
      "Average value: 643.1921172187151 for episode: 764\n",
      "Average value: 661.0325113577793 for episode: 765\n",
      "Average value: 677.9808857898903 for episode: 766\n",
      "Average value: 694.0818415003957 for episode: 767\n",
      "Average value: 709.3777494253759 for episode: 768\n",
      "Average value: 723.9088619541071 for episode: 769\n",
      "Average value: 737.7134188564017 for episode: 770\n",
      "Average value: 750.8277479135817 for episode: 771\n",
      "Average value: 763.2863605179026 for episode: 772\n",
      "Average value: 775.1220424920075 for episode: 773\n",
      "Average value: 786.3659403674071 for episode: 774\n",
      "Average value: 797.0476433490368 for episode: 775\n",
      "Average value: 807.1952611815849 for episode: 776\n",
      "Average value: 816.8354981225057 for episode: 777\n",
      "Average value: 825.9937232163803 for episode: 778\n",
      "Average value: 834.6940370555612 for episode: 779\n",
      "Average value: 842.9593352027831 for episode: 780\n",
      "Average value: 850.8113684426439 for episode: 781\n",
      "Average value: 858.2708000205117 for episode: 782\n",
      "Average value: 865.3572600194861 for episode: 783\n",
      "Average value: 872.0893970185118 for episode: 784\n",
      "Average value: 878.4849271675862 for episode: 785\n",
      "Average value: 884.5606808092068 for episode: 786\n",
      "Average value: 890.3326467687465 for episode: 787\n",
      "Average value: 895.8160144303091 for episode: 788\n",
      "Average value: 901.0252137087937 for episode: 789\n",
      "Average value: 905.9739530233539 for episode: 790\n",
      "Average value: 910.6752553721863 for episode: 791\n",
      "Average value: 915.1414926035769 for episode: 792\n",
      "Average value: 919.3844179733981 for episode: 793\n",
      "Average value: 923.4151970747281 for episode: 794\n",
      "Average value: 927.2444372209917 for episode: 795\n",
      "Average value: 930.882215359942 for episode: 796\n",
      "Average value: 934.3381045919449 for episode: 797\n",
      "Average value: 937.6211993623476 for episode: 798\n",
      "Average value: 940.7401393942303 for episode: 799\n",
      "Average value: 943.7031324245187 for episode: 800\n",
      "Average value: 946.5179758032928 for episode: 801\n",
      "Average value: 949.192077013128 for episode: 802\n",
      "Average value: 951.7324731624716 for episode: 803\n",
      "Average value: 954.145849504348 for episode: 804\n",
      "Average value: 956.4385570291306 for episode: 805\n",
      "Average value: 958.616629177674 for episode: 806\n",
      "Average value: 960.6857977187902 for episode: 807\n",
      "Average value: 962.6515078328507 for episode: 808\n",
      "Average value: 964.5189324412081 for episode: 809\n",
      "Average value: 966.2929858191477 for episode: 810\n",
      "Average value: 967.9783365281903 for episode: 811\n",
      "Average value: 969.5794197017808 for episode: 812\n",
      "Average value: 971.1004487166917 for episode: 813\n",
      "Average value: 972.5454262808571 for episode: 814\n",
      "Average value: 973.9181549668142 for episode: 815\n",
      "Average value: 975.2222472184734 for episode: 816\n",
      "Average value: 976.4611348575497 for episode: 817\n",
      "Average value: 977.6380781146722 for episode: 818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value: 978.7561742089385 for episode: 819\n",
      "Average value: 979.8183654984916 for episode: 820\n",
      "Average value: 980.827447223567 for episode: 821\n",
      "Average value: 981.7860748623885 for episode: 822\n",
      "Average value: 982.6967711192691 for episode: 823\n",
      "Average value: 983.5619325633056 for episode: 824\n",
      "Average value: 984.3838359351403 for episode: 825\n",
      "Average value: 985.1646441383832 for episode: 826\n",
      "Average value: 985.906411931464 for episode: 827\n",
      "Average value: 986.6110913348908 for episode: 828\n",
      "Average value: 987.2805367681462 for episode: 829\n",
      "Average value: 987.9165099297389 for episode: 830\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-156f3cc0a7c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplayBuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mstep_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mnoise_counter\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-1764f730af81>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_data)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# ---------------------- optimize critic ----------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mnext_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mq_expected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnext_val\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-3db410fc1b5c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sa)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0ms1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = NormalizeAction(env) # remap action values for the environment\n",
    "avg_val = 0\n",
    "\n",
    "#for plotting\n",
    "running_rewards_ddpg = []\n",
    "step_list_ddpg = []\n",
    "step_counter = 0\n",
    "\n",
    "# set term_condition for early stopping according to environment being used\n",
    "#term_condition = -150 # Pendulum\n",
    "#term_condition = 500 # inverted Pendulum\n",
    "term_condition = 1500 # half cheetah\n",
    "\n",
    "param_noise = AdaptiveParamNoiseSpec(initial_stddev=0.05,desired_action_stddev=0.3, adaptation_coefficient=1.05)\n",
    "\n",
    "for itr in range(NUM_EPISODES):\n",
    "    s=env.reset() # get initial state\n",
    "    animate_this_episode = (itr % animate_interval == 0) and VISUALIZE\n",
    "    total_reward=0\n",
    "    ddpg.perturb_actor_parameters(param_noise)\n",
    "    #actor_noise = OrnsteinUhlenbeckProcess(mu=np.zeros(act_dim))\n",
    "    noise_counter=0\n",
    "    while True:\n",
    "        ddpg.noise.reset()\n",
    "        if animate_this_episode:\n",
    "                env.render('rgb_array')\n",
    "                time.sleep(0.05)\n",
    "        a = ddpg.action(s, ddpg.noise.step(),param_noise)\n",
    "        #a = ddpg.action(s, None, param_noise)\n",
    "        s1, r, done, _ = env.step(a)\n",
    "        total_reward += r\n",
    "\n",
    "        ddpg.replayBuffer.add([np.reshape(s,(1,obs_dim)), np.reshape(a,(1,act_dim)), r, done, np.reshape(s1,(1,obs_dim))])\n",
    "        s = np.copy(s1)\n",
    "\n",
    "        training_data = np.array(ddpg.replayBuffer.sample(BATCH_SIZE))\n",
    "        ddpg.train(training_data)\n",
    "        step_counter += 1\n",
    "        noise_counter +=1\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    if ddpg.replayBuffer.position-noise_counter > 0:\n",
    "            noise_data=ddpg.replayBuffer.data[ddpg.replayBuffer.position-noise_counter:ddpg.replayBuffer.position]\n",
    "    else:\n",
    "           noise_data=ddpg.replayBuffer.data[ddpg.replayBuffer.position-noise_counter+60000:60000] \\\n",
    "            + ddpg.replayBuffer.data[0:ddpg.replayBuffer.position]\n",
    "    noise_data=np.array(noise_data)\n",
    "    noise_s = np.vstack(noise_data[:,0])\n",
    "    noise_a = np.vstack(noise_data[:,1])\n",
    "\n",
    "    perturbed_actions = noise_a\n",
    "    unperturbed_actions = ddpg.action(noise_s, None, None)\n",
    "    ddpg_dist = ddpg_distance_metric(perturbed_actions, unperturbed_actions)\n",
    "    \n",
    "    param_noise.adapt(ddpg_dist)\n",
    "    \n",
    "    if itr > 3 and avg_val > term_condition:\n",
    "            break\n",
    "    running_rewards_ddpg.append(total_reward) # return of this episode\n",
    "    step_list_ddpg.append(step_counter)\n",
    "\n",
    "    avg_val = avg_val * 0.95 + 0.05*running_rewards_ddpg[-1]\n",
    "    print(\"Average value: {} for episode: {}\".format(avg_val,itr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot rewards over multiple training runs \n",
    "This is provided to generate and plot results for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def numpy_ewma_vectorized_v2(data, window):\n",
    "\n",
    "    alpha = 2 /(window + 1.0)\n",
    "    alpha_rev = 1-alpha\n",
    "    n = data.shape[0]\n",
    "\n",
    "    pows = alpha_rev**(np.arange(n+1))\n",
    "\n",
    "    scale_arr = 1/pows[:-1]\n",
    "    offset = data[0]*pows[1:]\n",
    "    pw0 = alpha*alpha_rev**(n-1)\n",
    "\n",
    "    mult = data*pw0*scale_arr\n",
    "    cumsums = mult.cumsum()\n",
    "    out = offset + cumsums*scale_arr[::-1]\n",
    "    return out\n",
    "\n",
    "plt.figure()\n",
    "out = numpy_ewma_vectorized_v2(np.array(running_rewards_ddpg),20)\n",
    "step_list_ddpg = np.array(step_list_ddpg)\n",
    "\n",
    "plt.plot(step_list_ddpg, out)\n",
    "plt.title('Training reward over multiple runs')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_ewma_vectorized_v2(data, window):\n",
    "\n",
    "    alpha = 2 /(window + 1.0)\n",
    "    alpha_rev = 1-alpha\n",
    "    n = data.shape[0]\n",
    "\n",
    "    pows = alpha_rev**(np.arange(n+1))\n",
    "\n",
    "    scale_arr = 1/pows[:-1]\n",
    "    offset = data[0]*pows[1:]\n",
    "    pw0 = alpha*alpha_rev**(n-1)\n",
    "\n",
    "    mult = data*pw0*scale_arr\n",
    "    cumsums = mult.cumsum()\n",
    "    out = offset + cumsums*scale_arr[::-1]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ddpg inverted-pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_rewards_ddpg1=np.load('ddpg_invpendulum_reward.npy')\n",
    "step_list_ddpg1=np.load('ddpg_invpendulum_step.npy')\n",
    "plt.figure()\n",
    "out = numpy_ewma_vectorized_v2(np.array(running_rewards_ddpg1),20)\n",
    "step_list_ddpg = np.array(step_list_ddpg1)\n",
    "\n",
    "plt.plot(step_list_ddpg, out)\n",
    "plt.title('Training reward over multiple runs')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ddpg pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_rewards_ddpg2=np.load('ddpg_pendulum_reward.npy')\n",
    "step_list_ddpg2=np.load('ddpg_pendulum_step.npy')\n",
    "plt.figure()\n",
    "out = numpy_ewma_vectorized_v2(np.array(running_rewards_ddpg2),20)\n",
    "step_list_ddpg = np.array(step_list_ddpg2)\n",
    "\n",
    "plt.plot(step_list_ddpg, out)\n",
    "plt.title('Training reward over multiple runs')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ddpg halfcheetah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_rewards_ddpg3=np.load('ddpg_cheetah_reward.npy')\n",
    "step_list_ddpg3=np.load('ddpg_cheetah_step.npy')\n",
    "plt.figure()\n",
    "out = numpy_ewma_vectorized_v2(np.array(running_rewards_ddpg3),20)\n",
    "step_list_ddpg = np.array(step_list_ddpg3)\n",
    "\n",
    "plt.plot(step_list_ddpg, out)\n",
    "plt.title('Training reward over multiple runs')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "\n",
    "In this section you will implement REINFORCE, with modifications for batch training. It will be for use on both discrete and continous action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Parametrization\n",
    "\n",
    "Define a MLP which outputs a distribution over the action preferences given input state. For the discrete case, the MLP outputs the likelihood of each action (softmax) while for the continuous case, the output is the mean and standard deviation parametrizing the normal distribution from which the action is sampled.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Policy parametrizing model, MLP\n",
    "# ----------------------------------------------------\n",
    "# 1 or 2 hidden layers with a small number of units per layer (similar to DQN)\n",
    "# use ReLU for hidden layer activations\n",
    "# softmax as activation for output if discrete actions, linear for continuous control\n",
    "# for the continuous case, output_dim=2*act_dim (each act_dim gets a mean and std_dev)\n",
    "\n",
    "class mlp(nn.Module):\n",
    "    def __init__(self, Dim_state, num_outputs, disct):\n",
    "        super(mlp, self).__init__()\n",
    "        self.disct = disct\n",
    "        if self.disct == True:\n",
    "            self.fc1 = nn.Linear(Dim_state, 50)\n",
    "            self.fc2 = nn.Linear(50, 50)\n",
    "            self.fc3 = nn.Linear(50, num_outputs)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(Dim_state, 50)\n",
    "            self.fc2 = nn.Linear(50, 50)\n",
    "            self.f3 = nn.Linear(50, num_outputs)   \n",
    "            self.f3_ = nn.Linear(50, num_outputs)     \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.disct == True:\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return F.softmax(x, dim=1)\n",
    "        else: \n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            mu=self.f3(x)\n",
    "            sigma=self.f3_(x)\n",
    "            return [mu,sigma] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that samples an action from the policy distribtion parameters obtained as output of the MLP. The function should return the action and the log-probability (log_odds) of taking that action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(logit, disct):\n",
    "    # logit is the output of the softmax/linear layer\n",
    "    # discrete is a flag for the environment type\n",
    "    # Hint: use Categorical and Normal from torch.distributions to sample action and get the log-probability\n",
    "    # Note that log_probability in this case translates to ln(\\pi(a|s)) \n",
    "    if disct == True:\n",
    "        action_distribution=torch.distributions.Categorical(logit)\n",
    "        action = action_distribution.sample()\n",
    "        log_odds = action_distribution.log_prob(action)\n",
    "    else : # continuous \n",
    "        action_distribution = torch.distributions.normal.Normal(logit[0],F.softplus(logit[1]))\n",
    "        action = action_distribution.sample()\n",
    "        log_odds = action_distribution.log_prob(action)\n",
    "    return action, log_odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function update_policy that defines the loss function and updates the MLP according to the REINFORCE update rule (ref. slide 24 of Lec 7 or page 330 of Sutton and Barto (2018)). The update algorithm to be used below is slightly different: instead of updating the network at every time-step, we take the gradient of the loss averaged over a batch of timesteps (this is to make SGD more stable). We also use a baseline to reduce variance. \n",
    "\n",
    "The discount factor is set as 1 here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculated as \"reward to go\"\n",
    "def rewardtogo(rewards, gamma =1):\n",
    "    r2g = []\n",
    "    acc_r = 0\n",
    "    for r in reversed(rewards):\n",
    "        acc_r = acc_r * gamma + r\n",
    "        r2g.append(acc_r)\n",
    "    return r2g[::-1]\n",
    "\n",
    "def update_policy(paths, net):\n",
    "    # paths: a list of paths (complete episodes, used to calculate return at each time step)\n",
    "    # net: MLP object\n",
    "    \n",
    "    num_paths = len(paths)\n",
    "    rew_cums = []\n",
    "    log_odds = []\n",
    "\n",
    "    \n",
    "    for path in paths:\n",
    "        # rew_cums should record return at each time step for each path\n",
    "        rew_cums += rewardtogo(path['reward'])\n",
    "        # log_odds should record log_odds obtained at each timestep of path\n",
    "        log_odds += path['log_odds']\n",
    "        # calculated as \"reward to go\" \n",
    "\n",
    "    # make log_odds, rew_cums each a vector\n",
    "    rewards = np.array(rew_cums)\n",
    "    log_odds = np.array(log_odds)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5) # create baseline\n",
    "    # calculate policy loss and average over paths\n",
    "    policy_loss = -rewards.dot(log_odds)/ num_paths\n",
    "    \n",
    "    # take optimizer step\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.sum().backward()\n",
    "    optimizer.step() \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up environment and instantiate objects. Your algorithm is to be tested on one discrete and two continuous environments. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Environment\n",
    "\n",
    "#discrete environment:\n",
    "#env_name='CartPole-v0'\n",
    "\n",
    "#continous environments:\n",
    "env_name='InvertedPendulum-v1'\n",
    "#env_name = 'HalfCheetah-v1'\n",
    "\n",
    "\n",
    "# Make the gym environment\n",
    "env = gym.make(env_name)\n",
    "visualize = True\n",
    "animate=visualize\n",
    "learning_rate = 1e-3\n",
    "\n",
    "max_path_length=None\n",
    "min_timesteps_per_batch=2000\n",
    "\n",
    "# Set random seeds\n",
    "seed=0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Saving parameters\n",
    "logdir='./REINFORCE/'\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "if visualize:\n",
    "    if not os.path.exists(logdir):\n",
    "        os.mkdir(logdir)\n",
    "    env = gym.wrappers.Monitor(env, logdir, force=True, video_callable=lambda episode_id: episode_id%animate_interval==0)\n",
    "env._max_episodes_steps = min_timesteps_per_batch\n",
    "\n",
    "\n",
    "# Is this env continuous, or discrete?\n",
    "discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "\n",
    "# Get observation and action space dimensions\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "\n",
    "# Maximum length for episodes\n",
    "max_path_length = max_path_length or env.spec.max_episode_steps\n",
    "\n",
    "# Make network object (remember to pass in appropriate flags for the type of action space in use)\n",
    "# net = mlp(*args)\n",
    "\n",
    "net = mlp(Dim_state = obs_dim, num_outputs = act_dim, disct = discrete).type(FloatTensor)\n",
    "\n",
    "# Make optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run REINFORCE\n",
    "\n",
    "Run REINFORCE for CartPole, InvertedPendulum, and HalfCheetah. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_iter = 1000 \n",
    "min_timesteps_per_batch = 2000  # sets the batch size for updating network\n",
    "avg_reward = 0\n",
    "avg_rewards = []\n",
    "step_list_reinforce = []\n",
    "total_steps = 0\n",
    "episodes = 0\n",
    "\n",
    "for itr in range(n_iter): # loop for number of optimization steps\n",
    "    paths = []\n",
    "    steps = 0\n",
    "    \n",
    "    while True: # loop to get enough timesteps in this batch --> if episode ends this loop will restart till steps reaches limit\n",
    "        ob = env.reset()   \n",
    "        animate_this_episode = (itr % animate_interval == 0) and visualize\n",
    "        obs, acs, rews, log_odds = [], [], [], [] \n",
    "        obs.append(ob)\n",
    "\n",
    "        while True: # loop for episode inside batch\n",
    "            if animate_this_episode:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "            # get parametrized policy distribution from net using current state ob\n",
    "            net.eval()\n",
    "            var_ob = Variable(torch.unsqueeze(FloatTensor(ob),0), requires_grad=False)\n",
    "            distribution_parameters = net(var_ob)\n",
    "            net.train()\n",
    "            # sample action and get log-probability (log_odds) from distribution\n",
    "            cuda_tensor_ac, log_odd= sample_action(logit = distribution_parameters , disct = discrete)\n",
    "            ac = cuda_tensor_ac.data[0].cpu().numpy()\n",
    "            # step environment, record reward, next state\n",
    "            ob, rew, done, _ = env.step(ac)\n",
    "            # append to obs, acs, rewards, log_odds\n",
    "            obs.append(ob)\n",
    "            acs.append(ac)\n",
    "            rews.append(rew)\n",
    "            log_odds.append(log_odd)\n",
    "            \n",
    "            # if done, restart episode till min_timesteps_per_batch is reached     \n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                episodes = episodes + 1\n",
    "                break\n",
    "                \n",
    "        path = {\"observation\" : obs, \n",
    "                \"reward\" : np.array(rews), \n",
    "                \"action\" : (acs),\n",
    "                \"log_odds\" : log_odds}\n",
    "        \n",
    "        paths.append(path)\n",
    "        \n",
    "        if steps > min_timesteps_per_batch:\n",
    "            break \n",
    "\n",
    "    update_policy(paths, net)  # use all complete episodes (a batch of timesteps) recorded in this itr to update net\n",
    "\n",
    "    if itr == 0:\n",
    "        avg_reward = path['reward'].sum()\n",
    "    else:\n",
    "        avg_reward = avg_reward * 0.95 + 0.05 * path['reward'].sum()\n",
    "    \n",
    "    if avg_reward > 500:\n",
    "        break\n",
    "    \n",
    "    total_steps += steps\n",
    "    print(avg_reward,end='\\r')\n",
    "    avg_rewards.append(avg_reward)\n",
    "    step_list_reinforce.append(total_steps)\n",
    "    if itr % logging_interval == 0:\n",
    "        print('Average reward: {}'.format(avg_reward))\n",
    "   \n",
    "      \n",
    "env.close()\n",
    "#np.save('reinforce_cheetah_reward.npy', avg_rewards)\n",
    "#np.save('reinforce_cheetah_step.npy', step_list_reinforce)\n",
    "plt.plot(avg_rewards)\n",
    "plt.title('Training reward for <env> over multiple runs ')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Average reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('reinforce_invpendulum_reward.npy', avg_rewards)\n",
    "np.save('reinforce_invpendulum_step.npy', step_list_reinforce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avg_rewards)\n",
    "plt.title('Training reward for <inv> over multiple runs ')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Average reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS (15% extra)\n",
    "\n",
    "Compare average returns for CartPole (discrete action space) when using REINFORCE and DQN. Since in REINFORCE we update the network after a set number of steps instead of after every episode, plot the average rewards as a function of steps rather than episodes for both DQN and REINFORCE. You will need to make minor edits to your DQN code from the previous assignment to record average returns as a function of time_steps.\n",
    "\n",
    "Similarly, compare REINFORCE with DDPG on InvertedPendulum and HalfCheetah using steps for the x-axis.\n",
    "\n",
    "You may use the example code provided below as a reference for the graphs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # import your DQN and format your average returns as defined above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN and Reinforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "batch_size = 1000\n",
    "learning_rate = 0.01                   # learning rate\n",
    "env = gym.make('CartPole-v0')   \n",
    "#env = env.unwrapped\n",
    "action_size = env.action_space.n  \n",
    "state_size = env.observation_space.shape[0]   \n",
    "hidden_size = 64\n",
    "alpha_decay = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,hidden_size,state_size, action_size):\n",
    "        super(Net, self).__init__()\n",
    "        nn.Module.__init__(self)\n",
    "        self.dense1 = nn.Linear(state_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, action_size)\n",
    "     #   self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate,weight_decay=alpha_decay)    \n",
    "     #   self.loss_func = nn.MSELoss()  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = F.tanh(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value\n",
    "    \n",
    "class QNetwork(object):\n",
    "    def __init__(self,learning_rate, state_size, action_size, hidden_size, alpha_decay):\n",
    "        self.hidden_size=hidden_size\n",
    "        self.state_size=state_size\n",
    "        self.action_size=action_size\n",
    "        self.alpha_decay=alpha_decay\n",
    "        self.net = Net(self.hidden_size,self.state_size,self.action_size)\n",
    "        self.LR = learning_rate    \n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=self.LR) \n",
    "        self.loss_func = nn.MSELoss() \n",
    "        \n",
    "        \n",
    "    def learn(self, current_q_value, expected_q_values):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        loss = self.loss_func(current_q_value, expected_q_values)\n",
    "        loss.backward()\n",
    "        \n",
    "        for param in self.net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay():\n",
    "# Replay should also have an initialize method which creates a minimum buffer for \n",
    "# the initial episodes to generate minibatches.  \n",
    "    def __init__(self, max_size):\n",
    "        self.memory_counter = 0    \n",
    "        self.capacity = max_size\n",
    "        self.memory = []\n",
    "        self.gamma=0.99\n",
    "        \n",
    "    def initialize(self,init_length, envir=env):\n",
    "        state = envir.reset()\n",
    "        for i in range (init_length):\n",
    "            action = LongTensor([[env.action_space.sample()]])\n",
    "            next_state, reward, done, _ = envir.step(action.data[0][0].cpu().numpy())\n",
    "            \n",
    "            self.push((FloatTensor([state]),\n",
    "                     action,  \n",
    "                     FloatTensor([next_state]),\n",
    "                     FloatTensor([reward]),done))\n",
    "            if done:\n",
    "                state=envir.reset()\n",
    "            else: \n",
    "                state = next_state            \n",
    "        \n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        self.memory_counter += 1\n",
    "\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def generate_minibatch(self, DQN, targetDQN, BATCH_SIZE):\n",
    "        transitions = self.sample(BATCH_SIZE)\n",
    "        batch_state, batch_action, batch_next_state, batch_reward, batch_done = zip(*transitions)\n",
    "        batch_state = Variable(torch.cat( batch_state))\n",
    "        batch_action = Variable(torch.cat(batch_action))\n",
    "        batch_reward = Variable(torch.cat(batch_reward))\n",
    "        batch_next_state = Variable(torch.cat(batch_next_state))\n",
    "\n",
    "        \n",
    "        # current Q values are estimated by NN for all actions\n",
    "        current_q_values = DQN.net.forward(batch_state).gather(1, batch_action)\n",
    "        \n",
    "        # expected Q values are estimated from actions which gives maximum Q value\n",
    "        max_next_q_values= targetDQN.net.forward(batch_next_state).detach().max(1)[0]\n",
    "         \n",
    "        #if done:     y = reward\n",
    "        #if not done: y = reward + gamma*Qmax\n",
    "        choose=np.invert(np.array(batch_done))*max_next_q_values.data.cpu().numpy()\n",
    "        \n",
    "        expected_q_values = batch_reward + (self.gamma * Variable(FloatTensor(choose)))\n",
    "        expected_q_values = FloatTensor(expected_q_values.data.cpu().numpy()[:,None]) \n",
    "        return  current_q_values,expected_q_values \n",
    "    \n",
    "        # loss is measured from error between current and newly expected Q values\n",
    "        #loss = F.mse_loss(current_q_values, expected_q_values)\n",
    "        # backpropagation of loss to NN\n",
    "        #optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        #optimizer.step()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN = QNetwork(learning_rate, state_size, action_size, hidden_size, alpha_decay)\n",
    "targetDQN = QNetwork(learning_rate, state_size, action_size, hidden_size, alpha_decay)\n",
    "\n",
    "targetDQN.net.load_state_dict(DQN.net.state_dict())\n",
    "#targetDQN.net.dense1.weight=DQN.net.dense1.weight\n",
    "\n",
    "# set targetDQN weights to DQN weights\n",
    "# targetDQN.model.weights = DQN.model.weights (syntax given here is for representation purpose only)\n",
    "\n",
    "## Initialize Replay Buffer\n",
    "###################################\n",
    "## Populate the initial experience buffer\n",
    "###################################\n",
    "\n",
    "replay = Replay(max_size=10000)\n",
    "replay.initialize(init_length=1000, envir=env)\n",
    "if use_cuda:\n",
    "    DQN.net.cuda()\n",
    "    targetDQN.net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime parameters\n",
    "num_episodes = 1000            # max number of episodes to learn from\n",
    "gamma = 0.99                   # future reward discount\n",
    "max_steps = 500                # cut off simulation after this many steps\n",
    "# Exploration parameters\n",
    "min_epsilon = 0.01             # minimum exploration probability\n",
    "decay_rate = 5/num_episodes    # exponential decay rate for exploration prob\n",
    "returns = np.zeros(num_episodes)\n",
    "learn_step_counter=0\n",
    "avg_reward = 0\n",
    "avg_rewards=[]\n",
    "\n",
    "for ep in range(0, num_episodes):\n",
    "    paths=[]\n",
    "    steps=0\n",
    "    while True:\n",
    "        total_reward=0\n",
    "        # --> start episode \n",
    "        state = env.reset()\n",
    "        epsilon = min_epsilon + (1.0 - min_epsilon)*np.exp(-decay_rate*ep)\n",
    "        rews=[]\n",
    "        while True:\n",
    "\n",
    "            # explore/exploit and get action using DQN\n",
    "            if random.random() > epsilon:\n",
    "                action = DQN.net.forward(Variable(FloatTensor([state]),volatile=True).type(FloatTensor)).data.max(1)[1].view(1, 1)\n",
    "\n",
    "            else:\n",
    "                action = LongTensor([[random.randrange(2)]])\n",
    "            next_state, reward, done, _ = env.step(action.data[0][0].cpu().numpy())\n",
    "\n",
    "            total_reward=total_reward+reward\n",
    "            # perform action and record new_state, action, reward\n",
    "            # populate Replay experience buffer\n",
    "            replay.push((FloatTensor([state]),action,FloatTensor([next_state]),FloatTensor([reward]),done))\n",
    "\n",
    "            state = next_state\n",
    "            rews.append(reward)\n",
    "            steps+=1\n",
    "            if done:\n",
    "                #print(ep,   round(np.mean(returns[ep-99:ep]),0),end='\\r')\n",
    "                break\n",
    "        path={'reward':np.array(rews)}\n",
    "        paths.append(path)\n",
    "        if steps > 2000:\n",
    "            break\n",
    "    # I modified the output of the generate_minibatch so that I can directly use these two q-values to \n",
    "    # calculate loss and do optimization \n",
    "    current_q_value, expected_q_values = replay.generate_minibatch(DQN, targetDQN, batch_size)\n",
    "    DQN.learn(current_q_value,expected_q_values)\n",
    "    returns[ep] = total_reward\n",
    "    # set targetDQN weights to DQN weights\n",
    "    # update DQN (run one epoch of training per episode with generated minibatch of states and qvalues)\n",
    "    targetDQN.net.load_state_dict(DQN.net.state_dict())\n",
    "    if ep==0:\n",
    "        avg_reward = path['reward'].sum()\n",
    "    else:\n",
    "        avg_reward = avg_reward * 0.95 + 0.05 * path['reward'].sum()\n",
    "    total_steps+=steps\n",
    "    avg_rewards.append(avg_reward)\n",
    "    step_list_DQN.append(total_steps)\n",
    "\n",
    "    if ep % 100 == 0:\n",
    "        print('Average reward: {}'.format(avg_reward))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avg_rewards)\n",
    "plt.title('Training reward for <env> over multiple runs ')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Average reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('dqn.npy', avg_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_rewards_DQN=np.load('DQN_reward.npy')\n",
    "step_list_DQN=np.load('DQN_step.npy')\n",
    "\n",
    "\n",
    "plt.plot(step_list_reinforce, avg_rewards) \n",
    "plt.plot(step_list_DQN, running_rewards_DQN)\n",
    "plt.title('Training reward for <inverted pendulum> over multiple runs')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Cumulative reward')\n",
    "#plt.legend(['DDPG', 'REINFORCE']) \n",
    "plt.legend(['DQN', 'REINFORCE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG and Reinforce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Half Cheetah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rewards=np.load('reinforce_cheetah_reward.npy')\n",
    "step_list_reinforce=np.load('reinforce_cheetah_step.npy')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "out = numpy_ewma_vectorized_v2(np.array(running_rewards_ddpg3),20)\n",
    "plt.plot(step_list_ddpg3, out) \n",
    "plt.plot(step_list_reinforce, avg_rewards)\n",
    "#plt.plot(step_list_DQN, out)\n",
    "plt.title('Training reward for <cheetah> over multiple runs')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.legend(['DDPG', 'REINFORCE']) \n",
    "#plt.legend(['DQN', 'REINFORCE'])\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "out = numpy_ewma_vectorized_v2(np.array(running_rewards_ddpg1),20)\n",
    "plt.plot(step_list_ddpg1, out) \n",
    "plt.plot(step_list_reinforce, avg_rewards)\n",
    "#plt.plot(step_list_DQN, out)\n",
    "plt.title('Training reward for <inverted pendulum> over multiple runs')\n",
    "plt.xlabel('Number of steps')\n",
    "plt.ylabel('Cumulative reward')\n",
    "plt.legend(['DDPG', 'REINFORCE']) \n",
    "#plt.legend(['DQN', 'REINFORCE'])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
